{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJOM_lhQOKe"
      },
      "source": [
        "#импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "import urllib.request\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2cSeymwmjg9"
      },
      "source": [
        "#у каждого студента свой вариант\n",
        "VARIANT = 100\n",
        "VARIANT_URL = f'https://storage.yandexcloud.net/plates/tasks_spring_2021/{VARIANT}.csv'\n",
        "\n",
        "#создаем папку data и скачиваем файл с ссылками на изображения\n",
        "os.makedirs('data', exist_ok = True)\n",
        "urllib.request.urlretrieve(VARIANT_URL, f'data/{VARIANT}.csv');\n",
        "\n",
        "df = pd.read_csv(f'data/{VARIANT}.csv')\n",
        "\n",
        "#создаем папку images и скачиваем туда изображения\n",
        "os.makedirs('data/images', exist_ok = True)\n",
        "for url in tqdm(list(df['url'])[:]):\n",
        "    name = url.split('/')[-1]\n",
        "    path_img = f'data/images/{name}'\n",
        "    if not os.path.exists(path_img):\n",
        "        urllib.request.urlretrieve(url, path_img);\n",
        "#если кто-то из студентов оптимизирует этот код\n",
        "#и напишет параллельную загрузку изображений, пожалуйста, поделитесь им со своими однокурсниками"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrVqzg1Wxxty"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IsmtuTlOpIR"
      },
      "source": [
        "#устанавливаем open source проект nomeroff-net для распознавания номеров\n",
        "!pip3 install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!git clone https://github.com/ria-com/nomeroff-net.git\n",
        "!cd nomeroff-net ; git checkout v1.1; git clone https://github.com/youngwanLEE/centermask2.git\n",
        "!pip3 install 'git+https://github.com/ria-com/nomeroff-net.git@v1.1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtajGJZZk-ee"
      },
      "source": [
        "#нейронку можно запустить на GPU, либо на CPU\n",
        "import os\n",
        "import pickle\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # For CPU inference\n",
        "\n",
        "# dynamically grow the memory used on the GPU\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True \n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "set_session(sess)\n",
        "\n",
        "# Import all necessary libraries.\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#В https://github.com/ria-com/nomeroff-net/issues/88 рассказывается как получить confidence\n",
        "#Но функция get_acc устарела и при tensorflow>=2.3.* не работает, тут приведена исправленная функция\n",
        "def get_acc_(predicted, decode,region):\n",
        "    labels = []\n",
        "    detector = textDetector.detectors[int(textDetector.detectors_map[region])]\n",
        "    for text in decode:\n",
        "        labels.append(detector.text_to_labels(text))\n",
        "    loss = tf.keras.backend.ctc_batch_cost(\n",
        "        np.array(labels), np.array(predicted)[:, 2:, :], \n",
        "        np.array([[detector.label_length] for label in labels]), \n",
        "        np.array([[detector.max_text_len] for label in labels])\n",
        "    )\n",
        "    return  1 - loss\n",
        "\n",
        "def get_acc(predicted, decode, regions):\n",
        "    acc = []\n",
        "    for i, region in enumerate(regions):\n",
        "        if textDetector.detectors_map.get(region, None) is None or len(decode[i]) == 0:\n",
        "            acc.append([0])\n",
        "        else:\n",
        "            detector = textDetector.detectors[int(textDetector.detectors_map[region])]\n",
        "            _acc = get_acc_([predicted[i]], [decode[i]],region)\n",
        "            acc.append(_acc[0])\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJArGTRMPmib"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# NomeroffNet path\n",
        "NOMEROFF_NET_DIR = os.path.abspath('nomeroff-net/')\n",
        "sys.path.append(NOMEROFF_NET_DIR)\n",
        "\n",
        "# Import license plate recognition tools.\n",
        "from NomeroffNet import Detector\n",
        "from NomeroffNet import filters\n",
        "from NomeroffNet import RectDetector\n",
        "from NomeroffNet import OptionsDetector\n",
        "from NomeroffNet import TextDetector\n",
        "from NomeroffNet import textPostprocessing\n",
        "\n",
        "# load models\n",
        "rectDetector = RectDetector()\n",
        "\n",
        "optionsDetector = OptionsDetector()\n",
        "optionsDetector.load(\"latest\")\n",
        "\n",
        "# Initialize text detector.\n",
        "textDetector = TextDetector({\n",
        "    \"eu_ua_2004_2015\": {\n",
        "        \"for_regions\": [\"eu_ua_2015\", \"eu_ua_2004\"],\n",
        "        \"model_path\": \"latest\"\n",
        "    },\n",
        "    \"eu_ua_1995\": {\n",
        "        \"for_regions\": [\"eu_ua_1995\"],\n",
        "        \"model_path\": \"latest\"\n",
        "    },\n",
        "    \"eu\": {\n",
        "        \"for_regions\": [\"eu\"],\n",
        "        \"model_path\": \"latest\"\n",
        "    },\n",
        "    \"ru\": {\n",
        "        \"for_regions\": [\"ru\", \"eu-ua-fake-lnr\", \"eu-ua-fake-dnr\"],\n",
        "        \"model_path\": \"latest\" \n",
        "    },\n",
        "    \"kz\": {\n",
        "        \"for_regions\": [\"kz\"],\n",
        "        \"model_path\": \"latest\"\n",
        "    },\n",
        "    \"ge\": {\n",
        "        \"for_regions\": [\"ge\"],\n",
        "        \"model_path\": \"latest\"\n",
        "    }\n",
        "})\n",
        "\n",
        "\n",
        "nnet = Detector()\n",
        "nnet.loadModel(NOMEROFF_NET_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYJOa4cAQZX"
      },
      "source": [
        "if 'OCR' not in os.listdir():\n",
        "    os.mkdir('OCR')\n",
        "if 'cv_img_mask_npz' not in os.listdir():\n",
        "    os.mkdir('cv_img_mask_npz')\n",
        "images = list(filter(lambda x: x[0] !='.',os.listdir('data/images')))\n",
        "#теперь прогоним все наши изображения из папки images через нейронку\n",
        "for img_name in tqdm(images):\n",
        "    try:\n",
        "        #нейронка выдает довольно много информации, будем ее записывать в отдельный файл для каждого изображения\n",
        "        d = {}\n",
        "        d['img_name'] = img_name\n",
        "        img_path = 'data/images/'+img_name\n",
        "        img = mpimg.imread(img_path)\n",
        "        cv_img_mask = nnet.detect_mask([img])\n",
        "        #не знаю почему, но иногда нейронка выдает несколько масок для одного изображения\n",
        "        #в этих случаях считаю, что нейронка сломалась и не нашла ни одного номера\n",
        "        if len(cv_img_mask) != 1:\n",
        "            print(img_name,'--',len(cv_img_mask))\n",
        "            continue\n",
        "        #запишем координаты найденных номеров и другие параметры\n",
        "        arrPoints = rectDetector.detect(cv_img_mask[0])\n",
        "        d['arrPoints'] = arrPoints\n",
        "        zones = rectDetector.get_cv_zonesBGR(img, arrPoints)\n",
        "        d['zones'] = zones\n",
        "        regionIds, stateIds, countLines = optionsDetector.predict(zones)\n",
        "        d['regionIds'] = regionIds\n",
        "        d['stateIds'] = stateIds\n",
        "        d['countLines'] = countLines\n",
        "        regionNames = optionsDetector.getRegionLabels(regionIds)\n",
        "        d['regionNames'] = regionNames\n",
        "        textArr, b = textDetector.predict(zones, regionNames, countLines, return_acc=True);\n",
        "        textArr = textPostprocessing(textArr, regionNames)\n",
        "        #в textArr запишем распознанные номера\n",
        "        d['textArr'] = textArr\n",
        "        acc = np.array(get_acc(b, textArr, regionNames))\n",
        "        #в acc запишем confidence \n",
        "        d['acc'] = acc\n",
        "        #в папку OCR запишем наш словарь d\n",
        "        with open('OCR/' + img_name.split('.')[0] + '.pickle', 'wb') as f:\n",
        "            pickle.dump(d, f)\n",
        "        #в cv_img_mask_npz запишем макси с найденными номерами\n",
        "        np.savez_compressed('cv_img_mask_npz/'+ img_name.split('.')[0] + '.npz', cv_img_mask)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlykRCb7AQes"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "#преобразуем номер к стандартному виду\n",
        "def rename_plate(s):\n",
        "    dictionary = {\" \":\"\", \"y\":\"у\",'a':'а','b':'в','c':'с','e':'е','h':'н','k':'к','m':'м','o':'о','p':'р',\n",
        "                 't':'т','x':'х','д':'d'} \n",
        "    s = s.lower()\n",
        "    for key in dictionary.keys():\n",
        "        s = s.replace(key, dictionary[key])\n",
        "    return s.upper()\n",
        "\n",
        "#напишем функцию, которая определеяет площадь полигона \n",
        "def PolygonArea(corners):\n",
        "    n = len(corners) # of corners\n",
        "    area = 0.0\n",
        "    for i in range(n):\n",
        "        j = (i + 1) % n\n",
        "        area += corners[i][0] * corners[j][1]\n",
        "        area -= corners[j][0] * corners[i][1]\n",
        "    area = abs(area) / 2.0\n",
        "    return area"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0uEVZOfCXBL"
      },
      "source": [
        "#у нас может быть несколько номеров на изображении\n",
        "#в качестве ответа нам нужно указать номер главного автомобиля, который ближе всего расположен к камере\n",
        "#обычно у этого автомобиля номер имеет самую большую площадь на изображении\n",
        "#поэтому логика такая: отбираем все номера с confidence>0.5\n",
        "#среди всех полученных номеров выбираем самый большой номер\n",
        "for i in tqdm_notebook(df.index[:]):\n",
        "    img_name = df.loc[i,'id']\n",
        "    try:\n",
        "        with open('OCR/' + img_name.split('.')[0]+'.pickle', 'rb') as f:\n",
        "            ocr_dict = pickle.load(f)\n",
        "            arrPoints  = ocr_dict['arrPoints']\n",
        "            area = np.array(list(map(PolygonArea,arrPoints)))\n",
        "            acc = ocr_dict['acc'].reshape((1, -1))[0]\n",
        "            mask_threshold = acc > 0.5\n",
        "            filtred_acc = acc[mask_threshold]\n",
        "            filtred_area = area[mask_threshold]\n",
        "            filtered_plates = np.array(ocr_dict['textArr'])[mask_threshold]\n",
        "            if len(filtered_plates) == 0:\n",
        "                df.loc[i,'predict'] = ''\n",
        "            else:\n",
        "                plate = filtered_plates[filtred_area.argmax()]\n",
        "                plate = rename_plate(plate)\n",
        "                accuracy = filtred_acc[filtred_area.argmax()]\n",
        "                df.loc[i,'predict'] = plate\n",
        "                df.loc[i,'confidence'] = accuracy\n",
        "    except:\n",
        "        df.loc[i,'predict'] = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W4QNBGiAQ4T"
      },
      "source": [
        "df.to_csv('data/{VARIANT}_with_prediction_nn.csv',index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
